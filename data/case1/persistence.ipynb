{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "# from utils import freq\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.environ['http_proxy'] = '127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = '127.0.0.1:7890'\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "\n",
    "# 查找Top 10的关键词，全是英文\n",
    "from collections import Counter\n",
    "import re\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()  # 将文本分词为单词列表\n",
    "    clean_words = [word for word in words if word not in stopwords]  # 去除停用词\n",
    "    clean_text = ' '.join(clean_words)  # 将列表中的单词重新组合成文本\n",
    "    return clean_text\n",
    "def remove_newlines(text):\n",
    "    # 将换行符替换为空格\n",
    "    clean_text = text.replace('\\n', ' ')\n",
    "    return clean_text\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # 将匹配到的网址替换为空字符串\n",
    "    clean_text = url_pattern.sub('', text)\n",
    "    return clean_text\n",
    "def remove_after_at(text):\n",
    "    # 匹配@符号后面的单词的正则表达式\n",
    "    after_at_pattern = re.compile(r'@\\w+\\s?')\n",
    "    clean_text = after_at_pattern.sub('', text)\n",
    "    return clean_text\n",
    "def remove_punctuation(text):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return clean_text\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "english_stopwords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n",
    "    'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n",
    "    'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    "    'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
    "    'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
    "    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "    'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    "    'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "    'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now' ,'RT' ,'weibo', 'rt' ,'cctv' ,'time',\n",
    "    'daiichi' ,'the' ,'fukushima' ,'[#' ,'#]' ,'5th'\n",
    "]\n",
    "def preprocess_text(text, stopwords):\n",
    "    text = remove_newlines(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_after_at(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_stopwords(text, stopwords)\n",
    "    return text\n",
    "def find_top_n_words(text, n):\n",
    "    words = ' '.join(text).split()  # 将所有文本拼接成一个长字符串后分词\n",
    "    word_freq = Counter(words)  # 统计词频\n",
    "    top_n_words = word_freq.most_common(n)  # 获取词频最高的前n个单词\n",
    "    return top_n_words\n",
    "\n",
    "\n",
    "def get_word_freq_list(text):\n",
    "    text = text.apply(lambda x: preprocess_text(x, english_stopwords))\n",
    "    top_words = find_top_n_words(text, 10)\n",
    "    return top_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cal_RW_SCORE(A_posts, B_posts):\n",
    "    A_texts = get_word_freq_list(A_posts['text_trans'])\n",
    "    B_texts = get_word_freq_list(B_posts['text_trans'])\n",
    "    print(A_texts, B_texts)\n",
    "    A_word_list = [word[0] for word in A_texts]\n",
    "    B_word_list = [word[0] for word in B_texts]\n",
    "    embed_A = model.encode(A_word_list, convert_to_tensor=True)\n",
    "    embed_B = model.encode(B_word_list, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embed_A, embed_B)\n",
    "    cosine_scores = cosine_scores.cpu().numpy()\n",
    "    rw_res = []\n",
    "    for i in range(len(A_word_list)):\n",
    "        # print(f\"{A_word_list[i]} <--> {B_word_list[cosine_scores[i].argmax()]}, score: {cosine_scores[i].max()}\")\n",
    "        rw_res.append({\n",
    "            'source': A_word_list[i],\n",
    "            'target': B_word_list[cosine_scores[i].argmax()],\n",
    "            'score': cosine_scores[i].max()\n",
    "        })\n",
    "    if len(rw_res) == 0:\n",
    "        rw_score = 0\n",
    "    else:\n",
    "        rw_score = sum([item['score'] for item in rw_res]) / len(rw_res)\n",
    "    # print(f\"rw_score: {rw_score}\")\n",
    "    return rw_score, rw_res\n",
    "\n",
    "\n",
    "def cal_hashtag_score(A_posts, B_posts):\n",
    "    A_hashtags = find_hashtags(A_posts).tolist()\n",
    "    B_hashtags = find_hashtags(B_posts).tolist()\n",
    "    A_hashtags_list = [item for sublist in A_hashtags for item in sublist]\n",
    "    B_hashtags_list = [item for sublist in B_hashtags for item in sublist]\n",
    "    # print(f\"A_hashtags: {A_hashtags_list}\")\n",
    "    # print(f\"B_hashtags: {B_hashtags_list}\")\n",
    "    embed_A = model.encode(A_hashtags_list, convert_to_tensor=True)\n",
    "    embed_B = model.encode(B_hashtags_list, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embed_A, embed_B)\n",
    "    cosine_scores = cosine_scores.cpu().numpy()\n",
    "    hashtag_res = []\n",
    "    for i in range(len(A_hashtags_list)):\n",
    "        hashtag_res.append({\n",
    "            'source': A_hashtags_list[i],\n",
    "            'target': B_hashtags_list[cosine_scores[i].argmax()],\n",
    "            'score': cosine_scores[i].max()\n",
    "        })\n",
    "    if len(hashtag_res) == 0:\n",
    "        hashtag_score = 0\n",
    "    else:\n",
    "        hashtag_score = sum([item['score'] for item in hashtag_res]) / len(hashtag_res)\n",
    "    # print(f\"hashtag_score: {hashtag_score}\")\n",
    "    # print(hashtag_res)\n",
    "    return hashtag_score, hashtag_res\n",
    "\n",
    "\n",
    "def find_hashtags(df, col='text_trans'):\n",
    "    return df[col].str.findall(r'#\\w+#|\\B#\\w+\\b')\n",
    "\n",
    "\n",
    "def find_posts_with_url(df, col='text'):\n",
    "    return df[df[col].str.contains(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',na=False)]\n",
    "\n",
    "\n",
    "def find_same_url(df, col='text'):\n",
    "    url_list = []\n",
    "    post_id_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        url = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', row[col])\n",
    "        if url:\n",
    "            for u in url:\n",
    "                url_list.append(u)\n",
    "                post_id_list.append(row['post_id'])\n",
    "    return url_list, post_id_list\n",
    "\n",
    "\n",
    "def find_posts_mention_other_platform(df, platform, col='text'):\n",
    "    return df[df[col].str.contains(platform, na=False)]\n",
    "\n",
    "\n",
    "def find_posts_with_engagement(df, threshold, col=[\"cnt_retweet\", \"cnt_agree\", \"cnt_comment\"]):\n",
    "    return df[df[col] > threshold]\n",
    "\n",
    "\n",
    "def cal_cluster_factor(A, B, df_data, date, cycle, all_posts, all_users,debug = False):\n",
    "    A_posts = df_data[df_data['from'] == A]\n",
    "    B_posts = df_data[df_data['from'] == B]\n",
    "    end_time = pd.to_datetime(date).strftime('%Y-%m-%d')\n",
    "    start_time = pd.to_datetime(date) - pd.Timedelta(days=cycle)\n",
    "    start_time = start_time.strftime('%Y-%m-%d')\n",
    "    start_time_half = pd.to_datetime(date) - pd.Timedelta(days=cycle / 2)\n",
    "    start_time_half = start_time_half.strftime('%Y-%m-%d')\n",
    "    B_posts = B_posts[(B_posts['publish_time'] >= start_time_half) & (B_posts['publish_time'] <= end_time)]\n",
    "    print(f\"A: {A_posts.shape[0]} ,B: {B_posts.shape[0]}\")\n",
    "    ##############计算sal_factor######################\n",
    "    all_A_posts = all_posts[all_posts['from'] == A]\n",
    "    all_A_posts = all_A_posts[(all_A_posts['publish_time'] >= start_time) & (all_A_posts['publish_time'] <= end_time)]\n",
    "    all_B_posts = all_posts[all_posts['from'] == B]\n",
    "    all_B_posts = all_B_posts[(all_B_posts['publish_time'] >= start_time_half) & (all_B_posts['publish_time'] <= end_time)]\n",
    "    print(f\"all_A: {all_A_posts.shape[0]} ,all_B: {all_B_posts.shape[0]}\")\n",
    "    # 计算salience factor\n",
    "    if all_A_posts.shape[0] == 0 or all_B_posts.shape[0] == 0:\n",
    "        sal_factor = 0\n",
    "    else:\n",
    "        sal_factor = (A_posts.shape[0] * B_posts.shape[0]) / (all_A_posts.shape[0] * all_B_posts.shape[0])\n",
    "    print(f\"sal_factor: {sal_factor}\")\n",
    "    ##############计算A B的文本RW_SCORE #######################\n",
    "    rw_score, rw_res = cal_RW_SCORE(A_posts, B_posts)\n",
    "    # print(f\"rw_score: {rw_score}\")\n",
    "    ##############计算A B的文本Hashtag_score#######################\n",
    "    hashtag_score, hashtag_res = cal_hashtag_score(A_posts, B_posts)\n",
    "    # print(f\"hashtag_score: {hashtag_score}\")\n",
    "    ##############计算A B的SameURL #######################\n",
    "    A_posts_url = find_posts_with_url(A_posts)\n",
    "    B_posts_url = find_posts_with_url(B_posts)\n",
    "    A_posts_url_list,_ = find_same_url(A_posts_url)\n",
    "    B_posts_url_list,_ = find_same_url(B_posts_url)\n",
    "    same_url = list(set(A_posts_url_list).intersection(set(B_posts_url_list)))\n",
    "    same_url_count = len(same_url)\n",
    "    # print(f\"same_url_count: {same_url_count}\")\n",
    "    ##############计算A B的 DirectURL #######################\n",
    "    A_direct_url = A_posts['url'].dropna().tolist()\n",
    "    B_direct_url = B_posts['url'].dropna().tolist()\n",
    "    direct_url = list(set(A_direct_url).intersection(set(B_direct_url)))\n",
    "    direct_url_count = len(direct_url)\n",
    "    # print(f\"direct_url_count: {direct_url_count}\")\n",
    "    ##############计算A B的 Refer #######################\n",
    "    B_mention_A = find_posts_mention_other_platform(B_posts, A, 'text_trans')\n",
    "    B_mention_A_num = B_mention_A.shape[0]\n",
    "    # print(f\"B mention A: {B_mention_A_num}\")\n",
    "    ##############计算A B的 KOL Inf #######################\n",
    "    # 注意只计算A的KOL\n",
    "    Inf_posts = find_posts_with_engagement(A_posts, 100)\n",
    "    Inf_posts_num = Inf_posts.shape[0]\n",
    "    # 找出A_posts中的账号\n",
    "    A_user = A_posts['user_id'].drop_duplicates().tolist()\n",
    "    # 在self.all_users中找出这些账号\n",
    "    A_user_info = all_users[all_users['user_id'].isin(A_user)]\n",
    "    A_user_info['fan'] = A_user_info['fan'].astype(int)\n",
    "    # 找出粉丝数量大于500的账号\n",
    "    InfAccts = A_user_info[A_user_info['fan'] > 500].shape[0]\n",
    "    max_Inf_post = find_posts_with_engagement(all_A_posts, 100)\n",
    "    max_Inf_post_num = max_Inf_post.shape[0]\n",
    "    all_A_user = all_A_posts['user_id'].drop_duplicates().tolist()\n",
    "    all_A_user_info = all_users[all_users['user_id'].isin(all_A_user)]\n",
    "    all_A_user_info['fan'] = all_A_user_info['fan'].astype(int)\n",
    "    max_InfAccts = all_A_user_info[all_A_user_info['fan'] > 500].shape[0]\n",
    "    # print(f\"A engagement: {Inf_posts_num}, fan > 500: {InfAccts}, max_Inf_post: {max_Inf_post_num}, max_fan > 500: {max_InfAccts}\")\n",
    "    KOL_inf = Inf_posts_num / max_Inf_post_num + InfAccts / max_InfAccts\n",
    "    # print(f\"KOL_inf: {KOL_inf}\")\n",
    "    #######################################################\n",
    "    sim_con = 0.1 * rw_score + 0.3 * same_url_count + 0.5 * direct_url_count + 0.4 * B_mention_A_num + 0.1 * hashtag_score\n",
    "    # 计算指数\n",
    "    p = 1 - np.exp(-1 * sal_factor - KOL_inf * sim_con)\n",
    "    # print(f\"p: {p}\")\n",
    "    def print_data(debug = False):\n",
    "        if debug:\n",
    "            print(f\"A: {A_posts.shape[0]} ,B: {B_posts.shape[0]}\")\n",
    "            print(f\"all_A: {all_A_posts.shape[0]} ,all_B: {all_B_posts.shape[0]}\")\n",
    "            print(f\"sal_factor: {sal_factor}\")\n",
    "            print(f\"rw_score: {rw_score}\")\n",
    "            print(f\"hashtag_score: {hashtag_score}\")\n",
    "            print(f\"same_url_count: {same_url_count}\")\n",
    "            print(f\"direct_url_count: {direct_url_count}\")\n",
    "            print(f\"B mention A: {B_mention_A_num}\")\n",
    "            print(f\"A engagement: {Inf_posts_num}, fan > 500: {InfAccts}, max_Inf_post: {max_Inf_post_num}, max_fan > 500: {max_InfAccts}\")\n",
    "            print(f\"KOL_inf: {KOL_inf}\")\n",
    "            print(f\"p: {p}\")\n",
    "    print_data(debug)\n",
    "    return p, rw_res, hashtag_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16731, 19), (6904, 12))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_posts = pd.read_csv('all_posts.csv')\n",
    "df_all_accounts = pd.read_csv('all_accounts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['240_china_nuclear_pollution',\n",
       " '70_billion_japan_water',\n",
       " 'Great_Wave_Kanagawa',\n",
       " 'cooling_water_nuclear_wastewater',\n",
       " 'foreign_affairs_questions',\n",
       " 'japan_nuclear_wastewater',\n",
       " 'korean_...',\n",
       " 'radioactive_condemn_water',\n",
       " 'radioactive_pollution_japan_sea',\n",
       " 'sue_TEPCO_japan',\n",
       " 'treatment_japan_waste_nuclear']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_table = [\n",
    "    (0, '240_china_nuclear_pollution', 165),\n",
    "    (1, '70_billion_japan_water', 165),\n",
    "    (2, 'Great_Wave_Kanagawa', 49),\n",
    "    (3, 'cooling_water_nuclear_wastewater', 238),\n",
    "    (4, 'foreign_affairs_questions', 27),\n",
    "    (5, 'japan_nuclear_wastewater', 27),\n",
    "    (6, 'korean_...', 392),\n",
    "    (7, 'radioactive_condemn_water', 28),\n",
    "    (8, 'radioactive_pollution_japan_sea', 96),\n",
    "    (9, 'sue_TEPCO_japan', 202),\n",
    "    (10, 'treatment_japan_waste_nuclear', 63)\n",
    "]\n",
    "\n",
    "# 提取中间一列作为 Python 列表\n",
    "cluster_names = [item[1] for item in hash_table]\n",
    "cluster_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 364 ,B: 21\n",
      "all_A: 472 ,all_B: 67\n",
      "sal_factor: 0.24171515304831773\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m cycle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      7\u001b[0m df_data \u001b[38;5;241m=\u001b[39m df_all_posts[df_all_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m cluster]\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcal_cluster_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_all_posts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_all_accounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 170\u001b[0m, in \u001b[0;36mcal_cluster_factor\u001b[1;34m(A, B, df_data, date, cycle, all_posts, all_users, debug)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msal_factor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msal_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m##############计算A B的文本RW_SCORE #######################\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m rw_score, rw_res \u001b[38;5;241m=\u001b[39m \u001b[43mcal_RW_SCORE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_posts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_posts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# print(f\"rw_score: {rw_score}\")\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m##############计算A B的文本Hashtag_score#######################\u001b[39;00m\n\u001b[0;32m    173\u001b[0m hashtag_score, hashtag_res \u001b[38;5;241m=\u001b[39m cal_hashtag_score(A_posts, B_posts)\n",
      "Cell \u001b[1;32mIn[14], line 68\u001b[0m, in \u001b[0;36mcal_RW_SCORE\u001b[1;34m(A_posts, B_posts)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcal_RW_SCORE\u001b[39m(A_posts, B_posts):\n\u001b[0;32m     67\u001b[0m     A_texts \u001b[38;5;241m=\u001b[39m get_word_freq_list(A_posts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_trans\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 68\u001b[0m     B_texts \u001b[38;5;241m=\u001b[39m \u001b[43mget_word_freq_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB_posts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_trans\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(A_texts, B_texts)\n\u001b[0;32m     70\u001b[0m     A_word_list \u001b[38;5;241m=\u001b[39m [word[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m A_texts]\n",
      "Cell \u001b[1;32mIn[14], line 59\u001b[0m, in \u001b[0;36mget_word_freq_list\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_word_freq_list\u001b[39m(text):\n\u001b[1;32m---> 59\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menglish_stopwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     top_words \u001b[38;5;241m=\u001b[39m find_top_n_words(text, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_words\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\CP\\lib\\site-packages\\pandas\\core\\series.py:4908\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\CP\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\CP\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\CP\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\CP\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[14], line 59\u001b[0m, in \u001b[0;36mget_word_freq_list.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_word_freq_list\u001b[39m(text):\n\u001b[1;32m---> 59\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menglish_stopwords\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     60\u001b[0m     top_words \u001b[38;5;241m=\u001b[39m find_top_n_words(text, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_words\n",
      "Cell \u001b[1;32mIn[14], line 44\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text, stopwords)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text, stopwords):\n\u001b[1;32m---> 44\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mremove_newlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     text \u001b[38;5;241m=\u001b[39m remove_urls(text)\n\u001b[0;32m     46\u001b[0m     text \u001b[38;5;241m=\u001b[39m remove_after_at(text)\n",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m, in \u001b[0;36mremove_newlines\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_newlines\u001b[39m(text):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# 将换行符替换为空格\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     clean_text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_text\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "start_time = \"2021-04-20\"\n",
    "end_time = \"2021-04-29\"\n",
    "cluster = cluster_names[2]\n",
    "A = 'weibo'\n",
    "B = 'twitter'\n",
    "cycle = 10\n",
    "df_data = df_all_posts[df_all_posts['cluster'] == cluster]\n",
    "cal_cluster_factor(A,B,df_data,end_time,cycle,df_all_posts,df_all_accounts,debug = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crossplatform",
   "language": "python",
   "name": "crossplatform"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
